{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daacc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "class BilstmEncoder(nn.Module):   #nn is class in pytorch  #use `PascalCase` for class names in Python.\n",
    "  #class for bidirectional lstm encoder.\n",
    "  def __init__(self,vocab_size,embedding_dim,hidden_dim,num_layers,dropout,output_dim):      # ??constructor to initialize the parameters\n",
    "    super(BilstmEncoder,self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "    self.lstm      = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers,\n",
    "                            batch_first=True,#!by default, the input shape of lstm is sequence_length,batch_size,embedding_dim, but we want batch_size first as input shape is batch_size,sequence_length\n",
    "                            bidirectional = True,\n",
    "                            dropout       = dropout)\n",
    "    #Pytorch is case sensetive. All module names uses upper case letter.\n",
    "    #like nn.LSTM,nn.RNN\n",
    "    #input dimension= embedding_dim\n",
    "    #both ht and ct are of same size(num_layers*num_directions,batch_size,hidden_dim) \n",
    "    #number of direction is 2 for bidirectional lstm \n",
    "  \n",
    "    #number of layers=num_layers is the number of stacked LSTM layers (vertical depth).\n",
    "    #make sure layers are capitalized while calling, we will not use small letter like nn.LSTM or nn.Linear not nn.linear \n",
    "    self.dropout=nn.Dropout(dropout)\n",
    "    self.fc=nn.Linear(hidden_dim *2,output_dim)\n",
    "  def forward(self,x): # !size of x is (batch_Size,sequence_length)\n",
    "    #forward pass where actual calculation/computation happens.\n",
    "    embedded=self.embedding(x)                               #  self.lstm(embedded) returns a tuple: (output, (h_n, c_n))\n",
    "    output,(h_n,c_n)=self.lstm(embedded)\n",
    "    hidden=torch.cat((h_n[-2],h_n[-1]),dim=1) #hidden after concatentaion : (batch_size,hidden_dim *2)\n",
    "    \n",
    "    '''  for sequence labeling, we use output as input for next layer\n",
    "         for sequence classification, we use the concatenated hidden state as input for next layer\n",
    "         \n",
    "         1.h_n[-2] → last forward layer\n",
    "\n",
    "         2.h_n[-1] → last backward layer\n",
    "\n",
    "         '''\n",
    "    dropout = self.dropout(hidden) #after dropout, (batch_size,hidden_dim *2)\n",
    "    out     = self.fc(dropout) #after linear layer, (batch_size,output_dim)\n",
    "    return out ,hidden                                                                     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680d26a",
   "metadata": {},
   "source": [
    "\n",
    "**Summary of Tensor Shapes in the Model:**\n",
    "\n",
    "- **Input x:** Shape is (batch_size, sequence_length)\n",
    "- **After Embedding Layer:** Shape is (batch_size, sequence_length, embedding_dim)\n",
    "- **After LSTM Layer:**\n",
    "  - Output: Shape is (batch_size, sequence_length, hidden_dim * 2) (since it’s bidirectional)\n",
    "  - h_n: Shape is (num_layers * 2, batch_size, hidden_dim)\n",
    "- **After Concatenating Last Layer’s Forward and Backward h_n:** Shape is (batch_size, hidden_dim * 2)\n",
    "- **After Dropout:** Shape remains (batch_size, hidden_dim * 2)\n",
    "- **After Linear Layer:** Shape is (batch_size, output_dim)\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14dc914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Bahdanau (additive) attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim, attention_dim):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.encoder_attn = nn.Linear(encoder_hidden_dim, attention_dim) #encoder_attn: shape(encoder_hidden_dim,)\n",
    "        self.decoder_attn = nn.Linear(decoder_hidden_dim, attention_dim)\n",
    "        self.v = nn.Parameter(torch.rand(attention_dim))  #self.v: shape (attn_dim,) or (attn_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        # encoder_outputs: (batch, src_len, encoder_hidden_dim)\n",
    "        # decoder_hidden: (batch, decoder_hidden_dim)\n",
    "        batch_size, src_len, _ = encoder_outputs.size()\n",
    "        #get the batch size and source length from encoder outputs\n",
    "        #- means we ignore the third dimension\n",
    "        #batch_size is the number of samples in one batch (say 64 sentences)\n",
    "        #src_len is the length of the source sequence (say 50 words)\n",
    "        \n",
    "        # Repeat decoder hidden state src_len times\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        #repeat the decoder hidden state src_len times to match the source length\n",
    "        #unsqueeze(1) adds a new dimension at index 1, making it  batch_size, 1, decoder_hidden_dim\n",
    "        \n",
    "        # Calculate energy\n",
    "        energy = torch.tanh(self.encoder_attn(encoder_outputs) + self.decoder_attn(decoder_hidden)) # (batch, src_len, attn_dim)\n",
    "        #!(batch, src_len, attn_dim) @ (attn_dim,) = batch,src_len \n",
    "        energy = energy @ self.v    # (batch, src_len)\n",
    "        \n",
    "        attn_weights = F.softmax(energy, dim=1) # (batch, src_len)\n",
    "        \n",
    "        # Compute context vector\n",
    "        context              = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1) # (batch, encoder_hidden_dim)\n",
    "        #dimension of attn_weights      = batch, src_len,\n",
    "        #          #dimension of encoder_output = batch, src_len, encoder_hidden_dim\n",
    "        context=F.softmax(context,dim=-1)\n",
    "        return context, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "690cea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class KvAttention(nn.Module): \n",
    "  def __init__(self,encoder_hidden_dim,decoder_hidden_dim,attention_dim,):\n",
    "    super(KvAttention,self).__init__()\n",
    "    self.encoder_attn = nn.Linear(encoder_hidden_dim,attention_dim) #linear layer to make similar dimension of encoder hidden size and decoder hidden size\n",
    "    \n",
    "    self.decoder_attn = nn.Linear(decoder_hidden_dim,attention_dim)\n",
    "    self.v            = nn.Parameter(torch.rand(attention_dim))\n",
    "  def forward(self,encoder_outputs,decoder_hidden): \n",
    "      #encoder_outputs                            : (``batch,src_len,encoder_hidden_dim``)\n",
    "      #decoder_hidden                             : shape(`batch_size ,decoder_hidden_dim`)\n",
    "    batch_size,src_len, _ = encoder_outputs.size() #get the batch size and source length from encoder outputs\n",
    "    #- means we ignore the third dimension\n",
    "    #batch_size is the number of samples in one batch (say 64 sentences)\n",
    "    #src_len is the length of the source sequence (say 50 words)\n",
    "    decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1,src_len,1).repeat((1,src_len,1))\n",
    "    #repeat the decoder hidden state src_len times to match the source length\n",
    "    #unsqueeze(1) adds a new dimension at index 1, making it  batch_size, 1, decoder_hidden_dim\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86121da2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DotProductAttention(nn.Module): \n",
    "    def   __init__(self, hidden_dim)    : \n",
    "        super(DotProductAttention, self).__init__()\n",
    "        # Trainable skip connection (combine context and decoder_hidden)\n",
    "        self.skip_fc = nn.Linear(hidden_dim * 2, hidden_dim) #this line definesa linear trainable layer to combine context vector and decoder hidden state. \n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden): \n",
    "        \"\"\"\n",
    "        encoder_outputs: (batch, src_len, hidden_dim)\n",
    "        decoder_hidden : (batch, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Unsqueeze decoder_hidden for bmm\n",
    "        decoder_hidden_exp = decoder_hidden.unsqueeze(1)  # (batch, 1, hidden_dim)\n",
    "\n",
    "        # Compute dot product attention scores\n",
    "        attn_scores = torch.bmm(decoder_hidden_exp, encoder_outputs.transpose(1, 2))  # (batch, 1, src_len)\n",
    "\n",
    "        # Softmax over last dimension (src_lenSWW) to get attention weights\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # (batch, 1, src_len)\n",
    "\n",
    "        # Compute context vector as weighted sum of encoder outputs\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)  # (batch, 1, hidden_dim)\n",
    "        context = context.squeeze(1)  # (batch, hidden_dim)\n",
    "\n",
    "        # Optional: return context instead of output if you only need attention output\n",
    "        return output, attn_weights.squeeze(1)  # (batch, hidden_dim), (batch, src_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773c83d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#practice attention code\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class KvAtten(nn.Moudule): \n",
    "  def __init__(self,):\n",
    "    super(KvAtten,self).__init__()\n",
    "    self.encoder_attn = nn.Linear(encoder_hidden_dim,attention_dim) #this will convert the encoder hidden dimenstion into size of attention_dim\n",
    "    self.decoder_attn = nn.Linear(decoder_hidden_dim,attention_dim)  #this will convert the decoder hiddden dimension into size of attention_dim\n",
    "    self.v            = nn.Parameter(torch.rand(attention_dim))    #this is a learnable parameter\n",
    "  def forward(self,encoder_outputs,decoder_hidden): \n",
    "    \"\"\"\n",
    "    encoder_outputs: (batch,src_len,encoder_hidden_dim)\n",
    "     decoder_hidden:output )batch size, decoder_hidden_dim\"\"\"\n",
    "     batch_size,src_len,        _ = encoder_outputs.size()\n",
    "     decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1,src_len,1)   # B * S* H \n",
    "     attention_scores=torch.tanh(self.encoder_attn(encoder_outputs) +self.decoder_attn(decoder_hidden)) \n",
    "     \n",
    "     \n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e846d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Step 0: Assume context vector from attention\n",
    "# context shape: (batch, 1, hidden_dim)\n",
    "context = context.squeeze(1)  # (batch, hidden_dim)\n",
    "\n",
    "# Step 1: External memory (e.g., 1000 entries of hidden_dim size)\n",
    "# You can learn this, or compute from external documents using a BERT encoder\n",
    "external_memory = torch.randn(1000, hidden_dim)  # (memory_size, hidden_dim)\n",
    "\n",
    "# Step 2: Normalize (optional but good for cosine-like similarity)\n",
    "context_norm = F.normalize(context, p=2, dim=-1)            # (batch, hidden_dim)\n",
    "memory_norm = F.normalize(external_memory, p=2, dim=-1)     # (memory_size, hidden_dim)\n",
    "\n",
    "# Step 3: Compute similarity (dot product)\n",
    "# Result shape: (batch, memory_size)\n",
    "similarity_scores = torch.matmul(context_norm, memory_norm.T)\n",
    "\n",
    "# Step 4: Retrieve Top-k similar entries\n",
    "topk_vals, topk_indices = torch.topk(similarity_scores, k=5, dim=-1)\n",
    "\n",
    "# Step 5 (optional): Retrieve memory entries\n",
    "retrieved_memory = external_memory[topk_indices]  # shape: (batch, k, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b85ef8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LuongAttentionLSTMDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
    "        super(LuongAttentionLSTMDecoder, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # Luong attention\n",
    "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Combine attention context with LSTM output\n",
    "        self.concat = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "        # Final output layer\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_token, hidden, cell, encoder_outputs):\n",
    "        \"\"\"\n",
    "        input_token: [batch_size]\n",
    "        hidden: [num_layers, batch_size, hidden_dim]\n",
    "        cell: [num_layers, batch_size, hidden_dim]\n",
    "        encoder_outputs: [batch_size, src_len, hidden_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = input_token.size(0)\n",
    "\n",
    "        # Embed input token\n",
    "        embedded = self.dropout(self.embedding(input_token))  # [batch_size, emb_dim]\n",
    "        embedded = embedded.unsqueeze(1)  # [batch_size, 1, emb_dim]\n",
    "\n",
    "        # LSTM forward\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))  # output: [B, 1, H]\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_energy = torch.bmm(self.attn(output), encoder_outputs.transpose(1, 2))  # [B, 1, src_len]\n",
    "        attn_weights = F.softmax(attn_energy, dim=-1)  # [B, 1, src_len]\n",
    "\n",
    "        # Compute context vector\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)  # [B, 1, H]\n",
    "\n",
    "        # Combine LSTM output and context vector\n",
    "        rnn_context = torch.cat((output, context), dim=2)  # [B, 1, 2H]\n",
    "        concat_output = torch.tanh(self.concat(rnn_context))  # [B, 1, H]\n",
    "\n",
    "        # Predict output token\n",
    "        prediction = self.fc_out(concat_output.squeeze(1))  # [B, output_dim]\n",
    "\n",
    "        return prediction, hidden, cell, attn_weights\n",
    "#decoder code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a798653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignment 1\n",
    "class AttentionKv(nn.Module): \n",
    "  def   __init__(self,encoder_hidden_dim,decoder_hidden_dim,attention_dim): \n",
    "    super(AttentionKv,self).__init__()\n",
    "    self.encoder_attn = nn.Linear(encoder_hidden_dim,attention_dim)\n",
    "    self.decoder_attn = nn.Linear(decoder_hidden_dim,attention_dim)\n",
    "    self.v            = nn.Parameter(torch.rand(attention_dim))\n",
    "  def forward(self,encoder_outputs,decoder_hidden): \n",
    "    batch_size,src_len, _ = encoder_outputs.size()\n",
    "    decoder_hidden=decoder_hidden.unsqueeze(1).repeat(1,src_len,1)\n",
    "    attention_scores=torch.tanh(self.encoder_attn(encoder_outputs)+self.decoder_attn(decoder_hidden))\n",
    "    attention_scores=attention_scores @self.v\n",
    "    attn_weights=F.softmax(attention_scores,dim=1)\n",
    "    return  attn_weights  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b698decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size         = 2\n",
    "src_len            = 3\n",
    "encoder_hidden_dim = 4\n",
    "decoder_hidden_dim = 4\n",
    "attention_dim      = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59d27294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define size\n",
    "batch_size               = 3\n",
    "encoder_hidden_dim = 60\n",
    "decoder_hidden_dim = 60\n",
    "attention_dim      = 30\n",
    "src_len=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b39d069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1982, -0.5656, -0.0344, -0.4023, -1.0217,  0.6845, -1.0943, -0.1195,\n",
       "          0.2003,  0.6870, -0.3685, -0.2558,  1.8566, -1.0952,  1.5763,  1.2893,\n",
       "         -0.7791,  0.8882, -1.9549, -0.3953,  0.4467, -0.2103, -0.3323, -0.8470,\n",
       "         -1.5153, -0.0453,  1.5179,  0.9221, -0.1601, -1.2935,  1.5567, -0.1089,\n",
       "         -0.5952, -1.1361,  0.6588,  0.6821, -1.0666, -0.8660, -0.9400, -1.2721,\n",
       "         -1.4257, -1.0858,  0.2832, -0.0877,  0.5734, -0.0793,  0.0876, -0.4869,\n",
       "         -1.3830,  0.2053,  0.9705, -0.2412,  0.1817,  0.8301, -1.7060, -0.5976,\n",
       "         -0.1194, -1.3846, -0.3416,  0.1388],\n",
       "        [-1.9080, -1.7609,  0.8781, -0.7118,  1.1466,  2.2397,  0.6078, -1.3261,\n",
       "         -0.7604,  0.1885, -0.9655,  1.2914,  0.7544, -0.2592,  2.5983, -0.8389,\n",
       "          0.4845,  0.3244,  1.2643,  0.6301, -0.3412,  0.3711,  0.2503, -0.9494,\n",
       "         -0.6602,  1.0392, -0.2214,  0.5862,  0.8444,  0.9143, -0.6322,  0.0389,\n",
       "          1.1701, -0.6585,  1.4844, -0.2719,  0.0957, -0.7449, -0.0706,  1.0799,\n",
       "          0.8803, -0.4466,  0.6826,  0.1177, -0.7069,  1.4266, -0.7166,  0.4893,\n",
       "         -0.3461,  1.6345, -1.0692,  0.2770, -0.5372, -0.2227,  2.5154, -0.1344,\n",
       "         -0.9436,  0.3178,  1.1955, -0.8102],\n",
       "        [ 0.6112, -0.2706, -0.5143, -1.7956,  0.9390, -0.6413, -3.2028, -0.2003,\n",
       "          0.4764, -0.6454,  0.0488, -1.5554, -0.5921,  0.1286,  1.1616,  1.1167,\n",
       "         -0.7889, -1.1711, -1.5253,  0.0273,  0.4548, -1.0929,  1.6156,  0.7354,\n",
       "         -0.4115,  1.4646,  1.9330, -0.4911,  0.6213,  0.8452,  0.4552, -0.0719,\n",
       "          0.5398, -0.1973, -0.1415, -0.9860, -1.3596,  1.2150, -1.4390,  1.7235,\n",
       "          0.7248,  0.5682, -1.6279, -0.1733,  2.0277,  0.6276, -1.8650,  0.0779,\n",
       "         -0.8273, -2.0589, -0.2644, -1.3051, -0.7073, -0.0402, -1.4288, -0.9687,\n",
       "          0.6211, -0.4799,  0.3525, -0.7501]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample data\n",
    "encoder_outputs = torch.randn(batch_size,src_len,encoder_hidden_dim)\n",
    "decoder_hidden  = torch.randn(batch_size,decoder_hidden_dim)\n",
    "encoder_outputs\n",
    "decoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fbe6426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentionKv(\n",
       "  (encoder_attn): Linear(in_features=60, out_features=30, bias=True)\n",
       "  (decoder_attn): Linear(in_features=60, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AttentionKv(encoder_hidden_dim,decoder_hidden_dim,attention_dim)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ba87083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0023, 0.0051, 0.0007, 0.0009, 0.0070, 0.0055, 0.0025, 0.0032, 0.0009,\n",
       "         0.0010, 0.0053, 0.0064, 0.0015, 0.0081, 0.0011, 0.0350, 0.0134, 0.0128,\n",
       "         0.0056, 0.0014, 0.0002, 0.0038, 0.0065, 0.0211, 0.0027, 0.0023, 0.0351,\n",
       "         0.0149, 0.0019, 0.0076, 0.0012, 0.0185, 0.0021, 0.0080, 0.0056, 0.0436,\n",
       "         0.0101, 0.0002, 0.0420, 0.0069, 0.0031, 0.0044, 0.0081, 0.0144, 0.0016,\n",
       "         0.0377, 0.0046, 0.0081, 0.0005, 0.0015, 0.0022, 0.0093, 0.0075, 0.0196,\n",
       "         0.0004, 0.0080, 0.0136, 0.0080, 0.0002, 0.0010, 0.0031, 0.0027, 0.0056,\n",
       "         0.0007, 0.0422, 0.0060, 0.0372, 0.0054, 0.0022, 0.0960, 0.0045, 0.0036,\n",
       "         0.0008, 0.0067, 0.0272, 0.0107, 0.0079, 0.0012, 0.0007, 0.0011, 0.0779,\n",
       "         0.0011, 0.0725, 0.0065, 0.0029, 0.0038, 0.0112, 0.0031, 0.0063, 0.0045,\n",
       "         0.0029, 0.0011, 0.0021, 0.0046, 0.0043, 0.0017, 0.0097, 0.0022, 0.0035,\n",
       "         0.0050],\n",
       "        [0.0038, 0.0907, 0.0198, 0.0041, 0.0003, 0.0002, 0.0023, 0.0056, 0.0347,\n",
       "         0.0050, 0.0061, 0.0013, 0.0028, 0.0216, 0.0215, 0.0010, 0.0112, 0.0063,\n",
       "         0.0006, 0.0015, 0.0011, 0.0032, 0.0072, 0.0104, 0.0004, 0.0114, 0.0010,\n",
       "         0.0099, 0.0079, 0.0026, 0.0061, 0.0016, 0.0008, 0.0012, 0.0036, 0.0017,\n",
       "         0.0203, 0.0374, 0.0013, 0.0401, 0.0033, 0.0045, 0.0010, 0.0060, 0.0006,\n",
       "         0.0098, 0.0189, 0.0005, 0.0138, 0.0023, 0.0141, 0.0106, 0.0158, 0.0483,\n",
       "         0.0232, 0.0004, 0.0066, 0.0191, 0.0025, 0.0524, 0.0113, 0.0049, 0.0069,\n",
       "         0.0016, 0.0042, 0.0117, 0.0092, 0.0386, 0.0117, 0.0062, 0.0010, 0.0127,\n",
       "         0.0063, 0.0089, 0.0006, 0.0004, 0.0013, 0.0091, 0.0129, 0.0002, 0.0105,\n",
       "         0.0166, 0.0017, 0.0050, 0.0029, 0.0067, 0.0109, 0.0053, 0.0070, 0.0085,\n",
       "         0.0050, 0.0023, 0.0397, 0.0074, 0.0039, 0.0023, 0.0007, 0.0097, 0.0108,\n",
       "         0.0201],\n",
       "        [0.0016, 0.0011, 0.0083, 0.0041, 0.0096, 0.0030, 0.0010, 0.0028, 0.0100,\n",
       "         0.0574, 0.0054, 0.0256, 0.0235, 0.0028, 0.0034, 0.0287, 0.0016, 0.0278,\n",
       "         0.0015, 0.0029, 0.0141, 0.0004, 0.0033, 0.0167, 0.0067, 0.0017, 0.0025,\n",
       "         0.0082, 0.0043, 0.0022, 0.0029, 0.0029, 0.0025, 0.0210, 0.0035, 0.0019,\n",
       "         0.0049, 0.0018, 0.0035, 0.0054, 0.0109, 0.0017, 0.0017, 0.0188, 0.0015,\n",
       "         0.0603, 0.0051, 0.0375, 0.0323, 0.0126, 0.0059, 0.0663, 0.0019, 0.0075,\n",
       "         0.0054, 0.0085, 0.0031, 0.0029, 0.0016, 0.0441, 0.0009, 0.0200, 0.0038,\n",
       "         0.0344, 0.0023, 0.0041, 0.0045, 0.0017, 0.0007, 0.0027, 0.0038, 0.0018,\n",
       "         0.0185, 0.0069, 0.0046, 0.0003, 0.0006, 0.0021, 0.0129, 0.0110, 0.0032,\n",
       "         0.0003, 0.0040, 0.0023, 0.0548, 0.0020, 0.0039, 0.0286, 0.0530, 0.0206,\n",
       "         0.0058, 0.0031, 0.0007, 0.0033, 0.0068, 0.0008, 0.0010, 0.0037, 0.0005,\n",
       "         0.0018]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = model(encoder_outputs,decoder_hidden)\n",
    "attn_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbc89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assignment 2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
