{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e0ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "\n",
    "class BiLSTMEncoder(nn.Module): # nn is a clas in pytorch that provides base class for all neural network. [!#naming conv=PascalCase]\n",
    "    \"\"\"Bidirectional LSTM encoder for sequence-to-sequence models.\n",
    "\n",
    "    Encodes input sequences into hidden states and outputs, suitable for tasks like machine translation.\n",
    "    Uses a bidirectional LSTM to capture context from both directions, followed by a linear layer for output projection.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the input vocabulary.\n",
    "        embedding_dim (int): Dimension of token embeddings.\n",
    "        hidden_dim (int): Dimension of LSTM hidden states per direction.\n",
    "        num_layers (int): Number of LSTM layers.\n",
    "        dropout (float): Dropout probability for regularization.\n",
    "        output_dim (int): Dimension of the output (e.g., target vocabulary size for classification).\n",
    "\n",
    "    Attributes:\n",
    "        embedding (nn.Embedding): Token embedding layer.\n",
    "        lstm (nn.LSTM): Bidirectional LSTM layer.\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "        fc (nn.Linear): Linear layer to project concatenated hidden states.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, \n",
    "                 num_layers: int, dropout: float, output_dim: int):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "        # Initialize layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True, #input shape (batch_size,sequence_length, embedding_dim by default shape of lstm is sequence_length first.)\n",
    "            bidirectional=True, \n",
    "            dropout=dropout if num_layers > 1 else 0.0  # Dropout only if multiple layers\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  #as we are uing bidirectional LSTM, hidden_dim *2 \n",
    "\n",
    "        # Initialize weights for stability  \n",
    "        nn.init.xavier_uniform_(self.embedding.weight)  #xavier initialization for embedding.\n",
    "        for name, param in self.lstm.named_parameters(): #xavier initialization for LSTM weights\n",
    "            #name parameters means all the parameters of LSTM gates.\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Performs the forward pass of the encoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input token IDs, shape (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                - output: LSTM outputs, shape (batch_size, sequence_length, hidden_dim * 2).\n",
    "                - hidden: Final hidden states, shape (num_layers * 2, batch_size, hidden_dim).\n",
    "                - cell: Final cell states, shape (num_layers * 2, batch_size, hidden_dim).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If input tensor shape or dimensions are invalid.\n",
    "        \"\"\"\n",
    "        # Validate input\n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(f\"Expected input shape (batch_size, sequence_length), got {x.shape}\")\n",
    "        if not torch.all(x >= 0) or not torch.all(x < self.vocab_size):\n",
    "            raise ValueError(f\"Input token IDs must be in [0, {self.vocab_size}), got min {x.min()}, max {x.max()}\")\n",
    "\n",
    "        # Embed input: (batch_size, sequence_length) -> (batch_size, sequence_length, embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Apply LSTM: (batch_size, sequence_length, embedding_dim) -> \n",
    "        # (batch_size, sequence_length, hidden_dim * 2), (num_layers * 2, batch_size, hidden_dim)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # Concatenate final forward and backward hidden states: (batch_size, hidden_dim * 2)\n",
    "        final_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "\n",
    "        # Apply dropout and linear layer: (batch_size, hidden_dim * 2) -> (batch_size, output_dim)\n",
    "        dropout = self.dropout(final_hidden)\n",
    "        out = self.fc(dropout)\n",
    "\n",
    "        return out, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf541d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau-style additive attention for sequence-to-sequence models.\n",
    "\n",
    "    Computes attention scores between the decoder's hidden state and encoder outputs,\n",
    "    producing a context vector and attention weights for use in decoding.\n",
    "\n",
    "    Args:\n",
    "        encoder_hidden_dim (int): Dimension of the encoder's hidden states.\n",
    "        decoder_hidden_dim (int): Dimension of the decoder's hidden states.\n",
    "        attention_dim (int): Dimension of the attention mechanism's hidden layer.\n",
    "\n",
    "    Attributes:\n",
    "        encoder_attn (nn.Linear): Linear layer to project encoder outputs.\n",
    "        decoder_attn (nn.Linear): Linear layer to project decoder hidden state.\n",
    "        v (nn.Parameter): Parameter vector to compute attention scores.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_hidden_dim: int, decoder_hidden_dim: int, attention_dim: int):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.decoder_hidden_dim = decoder_hidden_dim\n",
    "        self.attention_dim = attention_dim\n",
    "\n",
    "        # Linear layers to project encoder and decoder states\n",
    "        self.encoder_attn = nn.Linear(encoder_hidden_dim, attention_dim, bias=False)\n",
    "        self.decoder_attn = nn.Linear(decoder_hidden_dim, attention_dim, bias=False)\n",
    "        \n",
    "        # Attention score parameter, initialized with Glorot initialization for stability\n",
    "        self.v = nn.Parameter(torch.empty(attention_dim))\n",
    "        nn.init.xavier_uniform_(self.v.unsqueeze(0))  # Shape: (1, attention_dim)\n",
    "\n",
    "    def forward(self, encoder_outputs: torch.Tensor, decoder_hidden: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Computes the attention context vector and weights.\n",
    "\n",
    "        Args:\n",
    "            encoder_outputs (torch.Tensor): Encoder hidden states, shape (batch_size, src_len, encoder_hidden_dim).\n",
    "            decoder_hidden (torch.Tensor): Decoder hidden state, shape (batch_size, decoder_hidden_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]:\n",
    "                - context: Context vector, shape (batch_size, encoder_hidden_dim).\n",
    "                - attn_weights: Attention weights, shape (batch_size, src_len).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If input tensor shapes or dimensions do not match expected values.\n",
    "        \"\"\"\n",
    "        # Validate input shapes\n",
    "        if encoder_outputs.dim() != 3:\n",
    "            raise ValueError(\n",
    "                f\"Expected encoder_outputs to be 3D (batch_size, src_len, encoder_hidden_dim), got {encoder_outputs.shape}\"\n",
    "            )\n",
    "        if decoder_hidden.dim() != 2:\n",
    "            raise ValueError(\n",
    "                f\"Expected decoder_hidden to be 2D (batch_size, decoder_hidden_dim), got {decoder_hidden.shape}\"\n",
    "            )\n",
    "\n",
    "        batch_size, src_len, enc_dim = encoder_outputs.size()\n",
    "        if enc_dim != self.encoder_hidden_dim:\n",
    "            raise ValueError(f\"Encoder hidden dimension mismatch: expected {self.encoder_hidden_dim}, got {enc_dim}\")\n",
    "        if decoder_hidden.size(1) != self.decoder_hidden_dim:\n",
    "            raise ValueError(f\"Decoder hidden dimension mismatch: expected {self.decoder_hidden_dim}, got {decoder_hidden.size(1)}\")\n",
    "        if batch_size != decoder_hidden.size(0):\n",
    "            raise ValueError(f\"Batch size mismatch: encoder_outputs {batch_size}, decoder_hidden {decoder_hidden.size(0)}\")\n",
    "\n",
    "        # Repeat decoder hidden state to match source length: (batch_size, decoder_hidden_dim) -> (batch_size, src_len, decoder_hidden_dim)\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "        # Compute energy: (batch_size, src_len, encoder_hidden_dim) -> (batch_size, src_len, attention_dim)\n",
    "        #                + (batch_size, src_len, decoder_hidden_dim) -> (batch_size, src_len, attention_dim)\n",
    "        energy = torch.tanh(self.encoder_attn(encoder_outputs) + self.decoder_attn(decoder_hidden))\n",
    "\n",
    "        # Compute attention scores: (batch_size, src_len, attention_dim) @ (attention_dim,) -> (batch_size, src_len)\n",
    "        attention_scores = torch.matmul(energy, self.v)\n",
    "\n",
    "        # Apply softmax to get attention weights: (batch_size, src_len)\n",
    "        attn_weights = F.softmax(attention_scores, dim=1)\n",
    "\n",
    "        # Compute context vector: (batch_size, 1, src_len) @ (batch_size, src_len, encoder_hidden_dim) -> (batch_size, 1, encoder_hidden_dim)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "\n",
    "        return context, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea41e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau-style additive attention for sequence-to-sequence models.\n",
    "\n",
    "    Computes attention scores between decoder hidden state and encoder outputs,\n",
    "    producing a context vector and attention weights.\n",
    "\n",
    "    Args:\n",
    "        encoder_hidden_dim (int): Dimension of encoder hidden states.\n",
    "        decoder_hidden_dim (int): Dimension of decoder hidden states.\n",
    "        attention_dim (int): Dimension of the attention mechanism's hidden layer.\n",
    "\n",
    "    Attributes:\n",
    "        W_enc (nn.Linear): Linear layer for encoder outputs.\n",
    "        W_dec (nn.Linear): Linear layer for decoder hidden state.\n",
    "        V (nn.Linear): Linear layer to compute attention scores.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_hidden_dim: int, decoder_hidden_dim: int, attention_dim: int):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.decoder_hidden_dim = decoder_hidden_dim\n",
    "        self.attention_dim = attention_dim\n",
    "\n",
    "        # Linear layers to project encoder and decoder states\n",
    "        self.W_enc = nn.Linear(encoder_hidden_dim, attention_dim, bias=False)\n",
    "        self.W_dec = nn.Linear(decoder_hidden_dim, attention_dim, bias=False)\n",
    "        self.V = nn.Linear(attention_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, encoder_outputs: torch.Tensor, decoder_hidden: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Computes attention context and weights.\n",
    "\n",
    "        Args:\n",
    "            encoder_outputs (torch.Tensor): Encoder hidden states, shape (batch_size, src_len, encoder_hidden_dim).\n",
    "            decoder_hidden (torch.Tensor): Decoder hidden state, shape (batch_size, decoder_hidden_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]:\n",
    "                - context: Context vector, shape (batch_size, encoder_hidden_dim).\n",
    "                - attn_weights: Attention weights, shape (batch_size, src_len).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If input shapes do not match expected dimensions.\n",
    "        \"\"\"\n",
    "        # Validate input shapes\n",
    "        if encoder_outputs.dim() != 3 or decoder_hidden.dim() != 2:\n",
    "            raise ValueError(\n",
    "                f\"Expected encoder_outputs to be 3D (batch_size, src_len, encoder_hidden_dim), got {encoder_outputs.shape}; \"\n",
    "                f\"Expected decoder_hidden to be 2D (batch_size, decoder_hidden_dim), got {decoder_hidden.shape}\"\n",
    "            )\n",
    "\n",
    "        batch_size, src_len, enc_dim = encoder_outputs.size()\n",
    "        if enc_dim != self.encoder_hidden_dim:\n",
    "            raise ValueError(f\"Encoder hidden dimension mismatch: expected {self.encoder_hidden_dim}, got {enc_dim}\")\n",
    "        if decoder_hidden.size(1) != self.decoder_hidden_dim:\n",
    "            raise ValueError(f\"Decoder hidden dimension mismatch: expected {self.decoder_hidden_dim}, got {decoder_hidden.size(1)}\")\n",
    "\n",
    "        # Project encoder outputs: (batch_size, src_len, encoder_hidden_dim) -> (batch_size, src_len, attention_dim)\n",
    "        enc_projection = self.W_enc(encoder_outputs)\n",
    "\n",
    "        # Project decoder hidden state: (batch_size, decoder_hidden_dim) -> (batch_size, 1, attention_dim)\n",
    "        dec_projection = self.W_dec(decoder_hidden).unsqueeze(1)\n",
    "\n",
    "        # Compute attention scores: (batch_size, src_len, attention_dim) + (batch_size, 1, attention_dim)\n",
    "        attention_scores = self.V(torch.tanh(enc_projection + dec_projection)).squeeze(2)  # (batch_size, src_len)\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = F.softmax(attention_scores, dim=1)  # (batch_size, src_len)\n",
    "\n",
    "        # Compute context vector: weighted sum of encoder outputs\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)  # (batch_size, encoder_hidden_dim)\n",
    "\n",
    "        return context, attn_weights\n",
    "\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"Decoder with additive attention for sequence-to-sequence models.\n",
    "\n",
    "    Processes one token at a time, using attention to focus on relevant encoder outputs.\n",
    "    Outputs predictions for the next token and updated LSTM states.\n",
    "\n",
    "    Args:\n",
    "        output_dim (int): Size of the target vocabulary.\n",
    "        embed_dim (int): Dimension of token embeddings.\n",
    "        encoder_hidden_dim (int): Dimension of encoder hidden states.\n",
    "        decoder_hidden_dim (int): Dimension of decoder hidden states.\n",
    "        attention_dim (int): Dimension of the attention mechanism's hidden layer.\n",
    "        dropout (float, optional): Dropout probability. Defaults to 0.1.\n",
    "\n",
    "    Attributes:\n",
    "        embedding (nn.Embedding): Token embedding layer.\n",
    "        attention (AdditiveAttention): Attention mechanism.\n",
    "        rnn (nn.LSTM): LSTM layer for decoding.\n",
    "        fc_out (nn.Linear): Output projection layer.\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim: int, embed_dim: int, encoder_hidden_dim: int, \n",
    "                 decoder_hidden_dim: int, attention_dim: int, dropout: float = 0.1):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.decoder_hidden_dim = decoder_hidden_dim\n",
    "        self.attention_dim = attention_dim\n",
    "\n",
    "        # Initialize layers\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.attention = AdditiveAttention(encoder_hidden_dim, decoder_hidden_dim, attention_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim + encoder_hidden_dim, decoder_hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(decoder_hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_token: torch.Tensor, decoder_hidden: torch.Tensor, \n",
    "                decoder_cell: torch.Tensor, encoder_outputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Performs one decoding step.\n",
    "\n",
    "        Args:\n",
    "            input_token (torch.Tensor): Input token IDs, shape (batch_size, 1).\n",
    "            decoder_hidden (torch.Tensor): Previous hidden state, shape (1, batch_size, decoder_hidden_dim).\n",
    "            decoder_cell (torch.Tensor): Previous cell state, shape (1, batch_size, decoder_hidden_dim).\n",
    "            encoder_outputs (torch.Tensor): Encoder outputs, shape (batch_size, src_len, encoder_hidden_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                - prediction: Logits for next token, shape (batch_size, output_dim).\n",
    "                - hidden: Updated hidden state, shape (1, batch_size, decoder_hidden_dim).\n",
    "                - cell: Updated cell state, shape (1, batch_size, decoder_hidden_dim).\n",
    "                - attn_weights: Attention weights, shape (batch_size, src_len).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If input shapes do not match expected dimensions.\n",
    "        \"\"\"\n",
    "        # Validate input shapes\n",
    "        if input_token.dim() != 2 or input_token.size(1) != 1:\n",
    "            raise ValueError(f\"Expected input_token shape (batch_size, 1), got {input_token.shape}\")\n",
    "        if decoder_hidden.dim() != 3 or decoder_hidden.size(0) != 1:\n",
    "            raise ValueError(f\"Expected decoder_hidden shape (1, batch_size, decoder_hidden_dim), got {decoder_hidden.shape}\")\n",
    "        if decoder_cell.shape != decoder_hidden.shape:\n",
    "            raise ValueError(f\"Expected decoder_cell shape to match decoder_hidden, got {decoder_cell.shape}\")\n",
    "        if encoder_outputs.dim() != 3:\n",
    "            raise ValueError(f\"Expected encoder_outputs shape (batch_size, src_len, encoder_hidden_dim), got {encoder_outputs.shape}\")\n",
    "\n",
    "        batch_size = input_token.size(0)\n",
    "\n",
    "        # Embed input token: (batch_size, 1) -> (batch_size, 1, embed_dim)\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "\n",
    "        # Compute attention: (batch_size, src_len, encoder_hidden_dim), (batch_size, decoder_hidden_dim)\n",
    "        # -> (batch_size, encoder_hidden_dim), (batch_size, src_len)\n",
    "        context, attn_weights = self.attention(encoder_outputs, decoder_hidden.squeeze(0))\n",
    "        context = context.unsqueeze(1)  # (batch_size, 1, encoder_hidden_dim)\n",
    "\n",
    "        # Concatenate embedding and context: (batch_size, 1, embed_dim + encoder_hidden_dim)\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        # LSTM: (batch_size, 1, embed_dim + encoder_hidden_dim) -> (batch_size, 1, decoder_hidden_dim)\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (decoder_hidden, decoder_cell))\n",
    "\n",
    "        # Predict next token: (batch_size, 1, decoder_hidden_dim) -> (batch_size, output_dim)\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "\n",
    "        return prediction, hidden, cell, attn_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
