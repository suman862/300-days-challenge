{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9007ac6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.23.5\n",
      "Pandas version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf1a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2a51547",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration class for managing hyperparameters and settings\n",
    "class Config:\n",
    "    \"\"\"Configuration class for Seq2Seq model hyperparameters and settings.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.batch_size = 32\n",
    "        self.embedding_dim = 128\n",
    "        self.hidden_dim = 256\n",
    "        self.num_layers = 2\n",
    "        self.dropout = 0.1\n",
    "        self.attention_dim = 128\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_epochs = 20\n",
    "        self.teacher_forcing_ratio = 0.5\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model_save_path = 'seq2seq_model.pth'\n",
    "        self.log_file = 'training.log'\n",
    "        self.src_lang = 'en'\n",
    "        self.tgt_lang = 'de'\n",
    "        self.max_seq_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9424010d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# DataLoader with custom collate function for padding and batching\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_dataloader\u001b[39m(split: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m, batch_size: \u001b[38;5;28mint\u001b[39m = \u001b[32m32\u001b[39m, max_seq_len: \u001b[38;5;28mint\u001b[39m = \u001b[32m50\u001b[39m) -> Tuple[\u001b[43mDataLoader\u001b[49m, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mdict\u001b[39m]: \n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create DataLoader for the specified dataset split.\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "# DataLoader with custom collate function for padding and batching\n",
    "def get_dataloader(split: str = 'train', batch_size: int = 32, max_seq_len: int = 50) -> Tuple[DataLoader, dict, dict]: \n",
    "    \"\"\"Create DataLoader for the specified dataset split.\"\"\"\n",
    "    try:\n",
    "        data_iter = Multi30k(split=split, language_pair=(Config().src_lang, Config().tgt_lang))\n",
    "        src_vocab = build_vocab(data_iter, tokenize_en, 0)\n",
    "        tgt_vocab = build_vocab(data_iter, tokenize_de, 1)\n",
    "\n",
    "        def collate_fn(batch):\n",
    "            src_batch, tgt_batch = [], []\n",
    "            for src, tgt in batch:\n",
    "                src_tokens = ['<sos>'] + tokenize_en(src)[:max_seq_len - 2] + ['<eos>']\n",
    "                tgt_tokens = ['<sos>'] + tokenize_de(tgt)[:max_seq_len - 2] + ['<eos>']\n",
    "                src_batch.append(torch.tensor([src_vocab[token] for token in src_tokens]))\n",
    "                tgt_batch.append(torch.tensor([tgt_vocab[token] for token in tgt_tokens]))\n",
    "            src_batch = pad_sequence(src_batch, padding_value=src_vocab['<pad>'], batch_first=True)\n",
    "            tgt_batch = pad_sequence(tgt_batch, padding_value=tgt_vocab['<pad>'], batch_first=True)\n",
    "            return src_batch, tgt_batch\n",
    "\n",
    "        return DataLoader(list(data_iter), batch_size=batch_size, collate_fn=collate_fn), src_vocab, tgt_vocab\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in data loading: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc05b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52933b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216f7cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e2ae19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. No GPU detected or accessible.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    print(f\"CUDA is available. GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. No GPU detected or accessible.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e0ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "\n",
    "class BiLSTMEncoder(nn.Module): # nn is a class in pytorch that provides base class for all neural network. [# !naming conv=PascalCase]\n",
    "    \"\"\"Bidirectional LSTM encoder for sequence-to-sequence models.\n",
    "\n",
    "    Encodes input sequences into hidden states and outputs, suitable for tasks like machine translation.\n",
    "    Uses a bidirectional LSTM to capture context from both directions, followed by a linear layer for output projection.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the input vocabulary.\n",
    "        embedding_dim (int): Dimension of token embeddings.\n",
    "        hidden_dim (int): Dimension of LSTM hidden states per direction.\n",
    "        num_layers (int): Number of LSTM layers.\n",
    "        dropout (float): Dropout probability for regularization.\n",
    "        output_dim (int): Dimension of the output (e.g., target vocabulary size for classification).\n",
    "\n",
    "    Attributes:\n",
    "        embedding (nn.Embedding): Token embedding layer.\n",
    "        lstm (nn.LSTM): Bidirectional LSTM layer.\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "        fc (nn.Linear): Linear layer to project concatenated hidden states.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, #constsructor to initialize layrs and args . \n",
    "                 num_layers: int, dropout: float, output_dim: int):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "        # Initialize layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True, #input shape (batch_size,sequence_length, embedding_dim) by default shape of lstm is sequence_length first.)\n",
    "            bidirectional=True, \n",
    "            dropout=dropout if num_layers > 1 else 0.0  # Dropout only if multiple layers\n",
    "        )\n",
    "        #Pytorch is case sensetive. All module names uses upper case letter.\n",
    "        #like nn.LSTM,nn.RNN\n",
    "        #input dimension= embedding_dim\n",
    "        #both ht and ct are of same size(num_layers*num_directions,batch_size,hidden_dim) \n",
    "        #number of direction is 2 for bidirectional lstm \n",
    "  \n",
    "        #number of layers=num_layers is the number of stacked LSTM layers (vertical depth).\n",
    "        #make sure layers are capitalized while calling, we will not use small letter like nn.LSTM or nn.Linear not nn.linear \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  #as we are uing bidirectional LSTM, hidden_dim *2 \n",
    "\n",
    "        # Initialize weights for stability  \n",
    "        nn.init.xavier_uniform_(self.embedding.weight)  #xavier initialization for embedding.\n",
    "        for name, param in self.lstm.named_parameters(): #xavier initialization for LSTM weights\n",
    "            #name parameters means all the parameters of LSTM gates.\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Performs the forward pass of the encoder(real calculation happens).\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input token IDs, shape (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                - output: LSTM outputs, shape (batch_size, sequence_length, hidden_dim * 2).\n",
    "                - hidden: Final hidden states, shape (num_layers * 2, batch_size, hidden_dim).\n",
    "                - cell: Final cell states, shape (num_layers * 2, batch_size, hidden_dim).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If input tensor shape or dimensions are invalid.\n",
    "        \"\"\"\n",
    "        # Validate input\n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(f\"Expected input shape (batch_size, sequence_length), got {x.shape}\")\n",
    "        if not torch.all(x >= 0) or not torch.all(x < self.vocab_size):\n",
    "            raise ValueError(f\"Input token IDs must be in [0, {self.vocab_size}), got min {x.min()}, max {x.max()}\")\n",
    "\n",
    "        # Embed input: (batch_size, sequence_length) -> (batch_size, sequence_length, embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Apply LSTM: (batch_size, sequence_length, embedding_dim) -> \n",
    "        # (batch_size, sequence_length, hidden_dim * 2), (num_layers * 2, batch_size, hidden_dim)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # Concatenate final forward and backward hidden states: (batch_size, hidden_dim * 2)\n",
    "        final_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "\n",
    "        # Apply dropout and linear layer: (batch_size, hidden_dim * 2) -> (batch_size, output_dim)\n",
    "        dropout = self.dropout(final_hidden)\n",
    "        out = self.fc(dropout)\n",
    "\n",
    "        return out, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf541d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau-style additive attention for sequence-to-sequence models.\n",
    "\n",
    "    Computes attention scores between the decoder's hidden state and encoder outputs,\n",
    "    producing a context vector and attention weights for use in decoding.\n",
    "\n",
    "    Args:\n",
    "        encoder_hidden_dim (int): Dimension of the encoder's hidden states.\n",
    "        decoder_hidden_dim (int): Dimension of the decoder's hidden states.\n",
    "        attention_dim (int): Dimension of the attention mechanism's hidden layer.\n",
    "\n",
    "    Attributes:\n",
    "        encoder_attn (nn.Linear): Linear layer to project encoder outputs.\n",
    "        decoder_attn (nn.Linear): Linear layer to project decoder hidden state.\n",
    "        v (nn.Parameter): Parameter vector to compute attention scores.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_hidden_dim: int, decoder_hidden_dim: int, attention_dim: int):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.decoder_hidden_dim = decoder_hidden_dim\n",
    "        self.attention_dim = attention_dim\n",
    "\n",
    "        # Linear layers to project encoder and decoder states\n",
    "        self.encoder_attn = nn.Linear(encoder_hidden_dim, attention_dim, bias=False)\n",
    "        self.decoder_attn = nn.Linear(decoder_hidden_dim, attention_dim, bias=False)\n",
    "        \n",
    "        # Attention score parameter, initialized with Glorot initialization for stability\n",
    "        self.v = nn.Parameter(torch.empty(attention_dim))\n",
    "        nn.init.xavier_uniform_(self.v.unsqueeze(0))  # Shape: (1, attention_dim)\n",
    "\n",
    "    def forward(self, encoder_outputs: torch.Tensor, decoder_hidden: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Computes the attention context vector and weights.\n",
    "\n",
    "        Args:\n",
    "            encoder_outputs (torch.Tensor): Encoder hidden states, shape (batch_size, src_len, encoder_hidden_dim).\n",
    "            decoder_hidden (torch.Tensor): Decoder hidden state, shape (batch_size, decoder_hidden_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]:\n",
    "                - context: Context vector, shape (batch_size, encoder_hidden_dim).\n",
    "                - attn_weights: Attention weights, shape (batch_size, src_len).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If input tensor shapes or dimensions do not match expected values.\n",
    "        \"\"\"\n",
    "        # Validate input shapes\n",
    "        if encoder_outputs.dim() != 3:\n",
    "            raise ValueError(\n",
    "                f\"Expected encoder_outputs to be 3D (batch_size, src_len, encoder_hidden_dim), got {encoder_outputs.shape}\"\n",
    "            )\n",
    "        if decoder_hidden.dim() != 2:\n",
    "            raise ValueError(\n",
    "                f\"Expected decoder_hidden to be 2D (batch_size, decoder_hidden_dim), got {decoder_hidden.shape}\"\n",
    "            )\n",
    "\n",
    "        batch_size, src_len, enc_dim = encoder_outputs.size()\n",
    "        if enc_dim != self.encoder_hidden_dim:\n",
    "            raise ValueError(f\"Encoder hidden dimension mismatch: expected {self.encoder_hidden_dim}, got {enc_dim}\")\n",
    "        if decoder_hidden.size(1) != self.decoder_hidden_dim:\n",
    "            raise ValueError(f\"Decoder hidden dimension mismatch: expected {self.decoder_hidden_dim}, got {decoder_hidden.size(1)}\")\n",
    "        if batch_size != decoder_hidden.size(0):\n",
    "            raise ValueError(f\"Batch size mismatch: encoder_outputs {batch_size}, decoder_hidden {decoder_hidden.size(0)}\")\n",
    "\n",
    "        # Repeat decoder hidden state to match source length: (batch_size, decoder_hidden_dim) -> (batch_size, src_len, decoder_hidden_dim)\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "        # Compute energy: (batch_size, src_len, encoder_hidden_dim) -> (batch_size, src_len, attention_dim)\n",
    "        #                + (batch_size, src_len, decoder_hidden_dim) -> (batch_size, src_len, attention_dim)\n",
    "        energy = torch.tanh(self.encoder_attn(encoder_outputs) + self.decoder_attn(decoder_hidden))\n",
    "\n",
    "        # Compute attention scores: (batch_size, src_len, attention_dim) @ (attention_dim,) -> (batch_size, src_len)\n",
    "        attention_scores = torch.matmul(energy, self.v)\n",
    "\n",
    "        # Apply softmax to get attention weights: (batch_size, src_len)\n",
    "        attn_weights = F.softmax(attention_scores, dim=1)\n",
    "\n",
    "        # Compute context vector: (batch_size, 1, src_len) @ (batch_size, src_len, encoder_hidden_dim) -> (batch_size, 1, encoder_hidden_dim)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "\n",
    "        return context, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea41e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"Decoder with additive attention for sequence-to-sequence models.\n",
    "\n",
    "    Processes one token at a time, using attention to focus on relevant encoder outputs.\n",
    "    Outputs predictions for the next token and updated LSTM states.\n",
    "\n",
    "    Args:\n",
    "        output_dim (int): Size of the target vocabulary.\n",
    "        embed_dim (int): Dimension of token embeddings.\n",
    "        encoder_hidden_dim (int): Dimension of encoder hidden states.\n",
    "        decoder_hidden_dim (int): Dimension of decoder hidden states.\n",
    "        attention_dim (int): Dimension of the attention mechanism's hidden layer.\n",
    "        dropout (float, optional): Dropout probability. Defaults to 0.1.\n",
    "\n",
    "    Attributes:\n",
    "        embedding (nn.Embedding): Token embedding layer.\n",
    "        attention (AdditiveAttention): Attention mechanism.\n",
    "        rnn (nn.LSTM): LSTM layer for decoding.\n",
    "        fc_out (nn.Linear): Output projection layer.\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim: int, embed_dim: int, encoder_hidden_dim: int, \n",
    "                 decoder_hidden_dim: int, attention_dim: int, dropout: float = 0.1):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.decoder_hidden_dim = decoder_hidden_dim\n",
    "        self.attention_dim = attention_dim\n",
    "\n",
    "        # Initialize layers\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.attention = AdditiveAttention(encoder_hidden_dim, decoder_hidden_dim, attention_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim + encoder_hidden_dim, decoder_hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(decoder_hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_token: torch.Tensor, decoder_hidden: torch.Tensor, \n",
    "                decoder_cell: torch.Tensor, encoder_outputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Performs one decoding step.\n",
    "\n",
    "        Args:\n",
    "            input_token (torch.Tensor): Input token IDs, shape (batch_size, 1).\n",
    "            decoder_hidden (torch.Tensor): Previous hidden state, shape (1, batch_size, decoder_hidden_dim).\n",
    "            decoder_cell (torch.Tensor): Previous cell state, shape (1, batch_size, decoder_hidden_dim).\n",
    "            encoder_outputs (torch.Tensor): Encoder outputs, shape (batch_size, src_len, encoder_hidden_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                - prediction: Logits for next token, shape (batch_size, output_dim).\n",
    "                - hidden: Updated hidden state, shape (1, batch_size, decoder_hidden_dim).\n",
    "                - cell: Updated cell state, shape (1, batch_size, decoder_hidden_dim).\n",
    "                - attn_weights: Attention weights, shape (batch_size, src_len).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If input shapes do not match expected dimensions.\n",
    "        \"\"\"\n",
    "        # Validate input shapes\n",
    "        if input_token.dim() != 2 or input_token.size(1) != 1:\n",
    "            raise ValueError(f\"Expected input_token shape (batch_size, 1), got {input_token.shape}\")\n",
    "        if decoder_hidden.dim() != 3 or decoder_hidden.size(0) != 1:\n",
    "            raise ValueError(f\"Expected decoder_hidden shape (1, batch_size, decoder_hidden_dim), got {decoder_hidden.shape}\")\n",
    "        if decoder_cell.shape != decoder_hidden.shape:\n",
    "            raise ValueError(f\"Expected decoder_cell shape to match decoder_hidden, got {decoder_cell.shape}\")\n",
    "        if encoder_outputs.dim() != 3:\n",
    "            raise ValueError(f\"Expected encoder_outputs shape (batch_size, src_len, encoder_hidden_dim), got {encoder_outputs.shape}\")\n",
    "\n",
    "        batch_size = input_token.size(0)\n",
    "\n",
    "        # Embed input token: (batch_size, 1) -> (batch_size, 1, embed_dim)\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "\n",
    "        # Compute attention: (batch_size, src_len, encoder_hidden_dim), (batch_size, decoder_hidden_dim)\n",
    "        # -> (batch_size, encoder_hidden_dim), (batch_size, src_len)\n",
    "        context, attn_weights = self.attention(encoder_outputs, decoder_hidden.squeeze(0))\n",
    "        context = context.unsqueeze(1)  # (batch_size, 1, encoder_hidden_dim)\n",
    "\n",
    "        # Concatenate embedding and context: (batch_size, 1, embed_dim + encoder_hidden_dim)\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        # LSTM: (batch_size, 1, embed_dim + encoder_hidden_dim) -> (batch_size, 1, decoder_hidden_dim)\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (decoder_hidden, decoder_cell))\n",
    "\n",
    "        # Predict next token: (batch_size, 1, decoder_hidden_dim) -> (batch_size, output_dim)\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "\n",
    "        return prediction, hidden, cell, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f25c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Sequence-to-Sequence model combining encoder and decoder.\"\"\"\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, teacher_forcing_ratio: float = 0.5) -> torch.Tensor:\n",
    "        batch_size, tgt_len = tgt.size()\n",
    "        outputs = torch.zeros(batch_size, tgt_len, self.decoder.output_dim).to(src.device)\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1).unsqueeze(0)\n",
    "        cell = torch.cat((cell[-2], cell[-1]), dim=1).unsqueeze(0)\n",
    "        input_token = tgt[:, 0:1]\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell, _ = self.decoder(input_token, hidden, cell, encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            input_token = tgt[:, t:t+1] if torch.rand(1).item() < teacher_forcing_ratio else output.argmax(1, keepdim=True)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24440cb0",
   "metadata": {},
   "source": [
    "FOR epoch in range(num_epochs): \n",
    "    SET model to training mode\n",
    "    INIT loss to 0\n",
    "    FOR each batch (src, tgt) in dataloader:\n",
    "        Move data to device\n",
    "        Zero out gradients\n",
    "        Forward pass through model â†’ Get output\n",
    "        Calculate loss using output and target\n",
    "        Backpropagate\n",
    "        Clip gradients\n",
    "        Update weights using optimizer\n",
    "        Track loss\n",
    "    END\n",
    "    Log average loss\n",
    "SAVE model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02814c19",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (1740028907.py, line 7)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m3model.train()  # Set to training mode (enables dropout)\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "def train_model(model: nn.Module, dataloader: DataLoader, optimizer: optim.Optimizer, \n",
    "                criterion: nn.Module, num_epochs: int, device: torch.device, save_path: str):\n",
    "    \"\"\"Train the Seq2Seq model with progress tracking and logging.\"\"\"\n",
    "    try:\n",
    "        model.to(device)  # Move model to GPU/CPU\n",
    "        for epoch in range(num_epochs):\n",
    "           model.train()  # Set to training mode (enables dropout)\n",
    "           epoch_loss = 0\n",
    "           with tqdm(total=len(dataloader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "                for src, tgt in dataloader:  # Iterate over batches\n",
    "                    src, tgt = src.to(device), tgt.to(device)  # Move data to device\n",
    "                    optimizer.zero_grad()  # Clear previous gradients\n",
    "                    output = model(src, tgt, Config().teacher_forcing_ratio)  # Forward pass\n",
    "                    loss = criterion(output[:, 1:].reshape(-1, model.decoder.output_dim), \n",
    "                                   tgt[:, 1:].reshape(-1))  # Compute loss\n",
    "                    loss.backward()  # Backpropagate\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
    "                    optimizer.step()  # Update weights\n",
    "                    epoch_loss += loss.item()  # Track loss\n",
    "                    pbar.update(1)  # Update progress bar\n",
    "                    pbar.set_postfix(loss=loss.item())  # Show current loss\n",
    "            avg_loss = epoch_loss / len(dataloader)  # Average loss per batch\n",
    "            logging.info(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}')  # Log result\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}')  # Print result\n",
    "        torch.save(model.state_dict(), save_path)  # Save model weights\n",
    "        logging.info(f'Model saved to {save_path}')  # Log save action\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training failed: {e}\")  # Log errors\n",
    "        raise  # Re-raise for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc21ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function for translation\n",
    "def translate_sentence(model: nn.Module, sentence: str, src_vocab, tgt_vocab, \n",
    "                      device: torch.device, max_length: int = 50) -> str:\n",
    "    \"\"\"Translate a sentence using the trained Seq2Seq model.\"\"\"\n",
    "    try:\n",
    "        model.eval()\n",
    "        tokens = ['<sos>'] + tokenize_en(sentence) + ['<eos>']\n",
    "        src_tensor = torch.tensor([src_vocab.get(token, src_vocab['<unk>']) for token in tokens], \n",
    "                                device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
    "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1).unsqueeze(0)\n",
    "            cell = torch.cat((cell[-2], cell[-1]), dim=1).unsqueeze(0)\n",
    "            input_token = torch.tensor([tgt_vocab['<sos>']], device=device).unsqueeze(0)\n",
    "            outputs = []\n",
    "            for _ in range(max_length):\n",
    "                output, hidden, cell, _ = model.decoder(input_token, hidden, cell, encoder_outputs)\n",
    "                pred_token = output.argmax(1).item()\n",
    "                if pred_token == tgt_vocab['<eos>']:\n",
    "                    break\n",
    "                outputs.append(pred_token)\n",
    "                input_token = torch.tensor([[pred_token]], device=device)\n",
    "        translation = ' '.join([tgt_vocab.get_itos()[idx] for idx in outputs])\n",
    "        return translation\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Inference failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b70f7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Main execution\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     args = \u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     config = Config()\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Load data\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mparse_args\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_args\u001b[39m():\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse command-line arguments.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     parser = \u001b[43margparse\u001b[49m.ArgumentParser(description=\u001b[33m'\u001b[39m\u001b[33mSeq2Seq Model Training and Inference\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m     parser.add_argument(\u001b[33m'\u001b[39m\u001b[33m--train\u001b[39m\u001b[33m'\u001b[39m, action=\u001b[33m'\u001b[39m\u001b[33mstore_true\u001b[39m\u001b[33m'\u001b[39m, help=\u001b[33m'\u001b[39m\u001b[33mTrain the model\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m     parser.add_argument(\u001b[33m'\u001b[39m\u001b[33m--infer\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m=\u001b[38;5;28mstr\u001b[39m, help=\u001b[33m'\u001b[39m\u001b[33mTranslate a sentence\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "# Command-line interface\n",
    "def parse_args():\n",
    "    \"\"\"Parse command-line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Seq2Seq Model Training and Inference')\n",
    "    parser.add_argument('--train', action='store_true', help='Train the model')\n",
    "    parser.add_argument('--infer', type=str, help='Translate a sentence')\n",
    "    parser.add_argument('--load_model', type=str, default=None, help='Path to pre-trained model')\n",
    "    return parser.parse_args()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    config = Config()\n",
    "    \n",
    "    # Load data\n",
    "    train_dataloader, src_vocab, tgt_vocab = get_dataloader(split='train', \n",
    "                                                          batch_size=config.batch_size, \n",
    "                                                          max_seq_len=config.max_seq_len)\n",
    "    \n",
    "    # Model instantiation\n",
    "    encoder = BiLSTMEncoder(\n",
    "        vocab_size=len(src_vocab),\n",
    "        embedding_dim=config.embedding_dim,\n",
    "        hidden_dim=config.hidden_dim,\n",
    "        num_layers=config.num_layers,\n",
    "        dropout=config.dropout\n",
    "    )\n",
    "    decoder = DecoderWithAttention(\n",
    "        output_dim=len(tgt_vocab),\n",
    "        embed_dim=config.embedding_dim,\n",
    "        encoder_hidden_dim=config.hidden_dim * 2,  # Bidirectional\n",
    "        decoder_hidden_dim=config.hidden_dim * 2,  # Match encoder's output\n",
    "        attention_dim=config.attention_dim,\n",
    "        dropout=config.dropout\n",
    "    )\n",
    "    model = Seq2Seq(encoder, decoder)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab['<pad>'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    # Load pre-trained model if specified\n",
    "    if args.load_model and os.path.exists(args.load_model):\n",
    "        model.load_state_dict(torch.load(args.load_model))\n",
    "        logging.info(f\"Loaded pre-trained model from {args.load_model}\")\n",
    "    \n",
    "    # Training\n",
    "    if args.train:\n",
    "        train_model(model, train_dataloader, optimizer, criterion, \n",
    "                   config.num_epochs, config.device, config.model_save_path)\n",
    "    \n",
    "    # Inference\n",
    "    if args.infer:\n",
    "        if not os.path.exists(config.model_save_path) and not args.load_model:\n",
    "            raise FileNotFoundError(\"No trained model found. Please train the model first or specify a pre-trained model path.\")\n",
    "        if args.load_model:\n",
    "            model.load_state_dict(torch.load(args.load_model))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(config.model_save_path))\n",
    "        translation = translate_sentence(model, args.infer, src_vocab, tgt_vocab, config.device)\n",
    "        print(f'Translation: {translation}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
