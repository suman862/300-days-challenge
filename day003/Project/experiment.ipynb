{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9007ac6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suman12/300-days-challenge/300-days-challenge/300cha/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/suman12/300-days-challenge/300-days-challenge/300cha/lib/python3.11/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import spacy\n",
    "import logging\n",
    "import os\n",
    "from typing import Tuple, List, Dict\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ebf1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class for hyperparameters and settings\n",
    "class Config:\n",
    "    \"\"\"Configuration for data processing and Seq2Seq model.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.batch_size = 32\n",
    "        self.max_seq_len = 50\n",
    "        self.embedding_dim = 128\n",
    "        self.hidden_dim = 256\n",
    "        self.num_layers = 2\n",
    "        self.dropout = 0.1\n",
    "        self.attention_dim = 128\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model_save_path = 'seq2seq_model.pth'\n",
    "        self.log_file = 'data_processing.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a51547",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename=Config().log_file, level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9424010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy for tokenization\n",
    "try:\n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "except OSError as e:\n",
    "    logging.error(f\"Failed to load SpaCy model: {e}\")\n",
    "    raise\n",
    "\n",
    "def tokenize_text(text: str) -> List[str]:\n",
    "    \"\"\"Tokenize text using SpaCy.\"\"\"\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30dc05b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading and preprocessing\n",
    "def load_and_preprocess_data(dataset_name: str = \"pubmed_qa\", split: str = \"train\", max_seq_len: int = 50) -> Tuple[DataLoader, Dict[str, int], Dict[str, int]]:\n",
    "    \"\"\"Load and preprocess a Hugging Face dataset for Seq2Seq training.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the Hugging Face dataset (e.g., 'pubmed_qa').\n",
    "        split (str): Dataset split ('train', 'validation', 'test').\n",
    "        max_seq_len (int): Maximum sequence length for source and target.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[DataLoader, Dict[str, int], Dict[str, int]]: DataLoader, source vocabulary, target vocabulary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load dataset from Hugging Face\n",
    "        dataset = load_dataset(dataset_name, 'pqa_labeled', split=split)\n",
    "        logging.info(f\"Loaded dataset {dataset_name} ({split} split) with {len(dataset)} samples\")\n",
    "\n",
    "        # Build vocabularies\n",
    "        def yield_tokens(data, key):\n",
    "            for sample in data:\n",
    "                yield tokenize_text(sample[key])\n",
    "\n",
    "        src_vocab = build_vocab_from_iterator(yield_tokens(dataset, 'question'), \n",
    "                                             specials=['<unk>', '<pad>', '<sos>', '<eos>'], min_freq=2)\n",
    "        tgt_vocab = build_vocab_from_iterator(yield_tokens(dataset, 'answer'), \n",
    "                                             specials=['<unk>', '<pad>', '<sos>', '<eos>'], min_freq=2)\n",
    "        src_vocab.set_default_index(src_vocab['<unk>'])\n",
    "        tgt_vocab.set_default_index(tgt_vocab['<unk>'])\n",
    "        logging.info(f\"Source vocab size: {len(src_vocab)}, Target vocab size: {len(tgt_vocab)}\")\n",
    "\n",
    "        # Collate function for DataLoader\n",
    "        def collate_fn(batch):\n",
    "            src_batch, tgt_batch = [], []\n",
    "            for sample in batch:\n",
    "                src_tokens = ['<sos>'] + tokenize_text(sample['question'])[:max_seq_len - 2] + ['<eos>']\n",
    "                tgt_tokens = ['<sos>'] + tokenize_text(sample['answer'])[:max_seq_len - 2] + ['<eos>']\n",
    "                src_batch.append(torch.tensor([src_vocab[token] for token in src_tokens]))\n",
    "                tgt_batch.append(torch.tensor([tgt_vocab[token] for token in tgt_tokens]))\n",
    "            src_batch = pad_sequence(src_batch, padding_value=src_vocab['<pad>'], batch_first=True)\n",
    "            tgt_batch = pad_sequence(tgt_batch, padding_value=tgt_vocab['<pad>'], batch_first=True)\n",
    "            return src_batch, tgt_batch\n",
    "\n",
    "        dataloader = DataLoader(dataset, batch_size=Config().batch_size, collate_fn=collate_fn)\n",
    "        return dataloader, src_vocab, tgt_vocab\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load or preprocess dataset: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b52933b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for image preprocessing (for multimodal extension)\n",
    "def preprocess_image_data(image_paths: List[str], processor=None) -> torch.Tensor:\n",
    "    \"\"\"Placeholder for preprocessing medical images (e.g., CheXpert).\n",
    "\n",
    "    Args:\n",
    "        image_paths (List[str]): List of image file paths.\n",
    "        processor: Image processor (e.g., from Hugging Face Transformers).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Processed image tensors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Example: Use a processor like CLIPProcessor for MedGemma-like models\n",
    "        if processor is None:\n",
    "            raise ValueError(\"Image processor not provided for multimodal data\")\n",
    "        # Implement image loading and processing here (e.g., using PIL or torchvision)\n",
    "        # For now, return a dummy tensor\n",
    "        logging.info(\"Image preprocessing placeholder executed\")\n",
    "        return torch.zeros(len(image_paths), 3, 224, 224)  # Dummy tensor (batch, channels, height, width)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Image preprocessing failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "216f7cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 22 12:23:32 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:01:00.0 Off |                  Off |\n",
      "|  0%   31C    P8              21W / 480W |    157MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      9065      G   /usr/lib/xorg/Xorg                           71MiB |\n",
      "|    0   N/A  N/A      9547      G   /usr/bin/gnome-shell                         54MiB |\n",
      "|    0   N/A  N/A    328673      G   /usr/bin/nautilus                            17MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99e2ae19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n",
      "Device Count: 1\n",
      "CUDA is NOT available. Check driver or GPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Device Count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is NOT available. Check driver or GPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a0e0ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "\n",
    "class BiLSTMEncoder(nn.Module): # nn is a class in pytorch that provides base class for all neural network. \n",
    "    \"\"\"Bidirectional LSTM encoder for sequence-to-sequence models.\n",
    "\n",
    "    Encodes input sequences into hidden states and outputs, suitable for tasks like machine translation.\n",
    "    Uses a bidirectional LSTM to capture context from both directions, followed by a linear layer for output projection.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the input vocabulary.\n",
    "        embedding_dim (int): Dimension of token embeddings.\n",
    "        hidden_dim (int): Dimension of LSTM hidden states per direction.\n",
    "        num_layers (int): Number of LSTM layers.\n",
    "        dropout (float): Dropout probability for regularization.\n",
    "        output_dim (int): Dimension of the output (e.g., target vocabulary size for classification).\n",
    "\n",
    "    Attributes:\n",
    "        embedding (nn.Embedding): Token embedding layer.\n",
    "        lstm (nn.LSTM): Bidirectional LSTM layer.\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "        fc (nn.Linear): Linear layer to project concatenated hidden states.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, #constsructor to initialize layrs and args . \n",
    "                 num_layers: int, dropout: float, output_dim: int):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "        # Initialize layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True, #input shape (batch_size,sequence_length, embedding_dim) by default shape of lstm is sequence_length first.)\n",
    "            bidirectional=True, \n",
    "            dropout=dropout if num_layers > 1 else 0.0  # Dropout only if multiple layers\n",
    "        )\n",
    "        #Pytorch is case sensetive. All module names uses upper case letter.\n",
    "        #like nn.LSTM,nn.RNN\n",
    "        #input dimension= embedding_dim\n",
    "        #both ht and ct are of same size(num_layers*num_directions,batch_size,hidden_dim) \n",
    "        #number of direction is 2 for bidirectional lstm \n",
    "  \n",
    "        #number of layers=num_layers is the number of stacked LSTM layers (vertical depth).\n",
    "        #make sure layers are capitalized while calling, we will not use small letter like nn.LSTM or nn.Linear not nn.linear \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  #as we are uing bidirectional LSTM, hidden_dim *2 \n",
    "\n",
    "        # Initialize weights for stability  \n",
    "        nn.init.xavier_uniform_(self.embedding.weight)  #xavier initialization for embedding.\n",
    "        for name, param in self.lstm.named_parameters(): #xavier initialization for LSTM weights\n",
    "            #name parameters means all the parameters of LSTM gates.\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Performs the forward pass of the encoder(real calculation happens).\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input token IDs, shape (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                - output: LSTM outputs, shape (batch_size, sequence_length, hidden_dim * 2).\n",
    "                - hidden: Final hidden states, shape (num_layers * 2, batch_size, hidden_dim).\n",
    "                - cell: Final cell states, shape (num_layers * 2, batch_size, hidden_dim).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If input tensor shape or dimensions are invalid.\n",
    "        \"\"\"\n",
    "        # Validate input\n",
    "        if x.dim() != 2:\n",
    "            raise ValueError(f\"Expected input shape (batch_size, sequence_length), got {x.shape}\")\n",
    "        if not torch.all(x >= 0) or not torch.all(x < self.vocab_size):\n",
    "            raise ValueError(f\"Input token IDs must be in [0, {self.vocab_size}), got min {x.min()}, max {x.max()}\")\n",
    "\n",
    "        # Embed input: (batch_size, sequence_length) -> (batch_size, sequence_length, embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Apply LSTM: (batch_size, sequence_length, embedding_dim) -> \n",
    "        # (batch_size, sequence_length, hidden_dim * 2), (num_layers * 2, batch_size, hidden_dim)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # Concatenate final forward and backward hidden states: (batch_size, hidden_dim * 2)\n",
    "        final_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "\n",
    "        # Apply dropout and linear layer: (batch_size, hidden_dim * 2) -> (batch_size, output_dim)\n",
    "        dropout = self.dropout(final_hidden)\n",
    "        out = self.fc(dropout)\n",
    "\n",
    "        return out, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbf541d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau-style additive attention for sequence-to-sequence models.\n",
    "\n",
    "    Computes attention scores between the decoder's hidden state and encoder outputs,\n",
    "    producing a context vector and attention weights for use in decoding.\n",
    "\n",
    "    Args:\n",
    "        encoder_hidden_dim (int): Dimension of the encoder's hidden states.\n",
    "        decoder_hidden_dim (int): Dimension of the decoder's hidden states.\n",
    "        attention_dim (int): Dimension of the attention mechanism's hidden layer.\n",
    "\n",
    "    Attributes:\n",
    "        encoder_attn (nn.Linear): Linear layer to project encoder outputs.\n",
    "        decoder_attn (nn.Linear): Linear layer to project decoder hidden state.\n",
    "        v (nn.Parameter): Parameter vector to compute attention scores.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_hidden_dim: int, decoder_hidden_dim: int, attention_dim: int):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.decoder_hidden_dim = decoder_hidden_dim\n",
    "        self.attention_dim = attention_dim\n",
    "\n",
    "        # Linear layers to project encoder and decoder states\n",
    "        self.encoder_attn = nn.Linear(encoder_hidden_dim, attention_dim, bias=False)\n",
    "        self.decoder_attn = nn.Linear(decoder_hidden_dim, attention_dim, bias=False)\n",
    "        \n",
    "        # Attention score parameter, initialized with Glorot initialization for stability\n",
    "        self.v = nn.Parameter(torch.empty(attention_dim))\n",
    "        nn.init.xavier_uniform_(self.v.unsqueeze(0))  # Shape: (1, attention_dim)\n",
    "\n",
    "    def forward(self, encoder_outputs: torch.Tensor, decoder_hidden: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Computes the attention context vector and weights.\n",
    "\n",
    "        Args:\n",
    "            encoder_outputs (torch.Tensor): Encoder hidden states, shape (batch_size, src_len, encoder_hidden_dim).\n",
    "            decoder_hidden (torch.Tensor): Decoder hidden state, shape (batch_size, decoder_hidden_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]:\n",
    "                - context: Context vector, shape (batch_size, encoder_hidden_dim).\n",
    "                - attn_weights: Attention weights, shape (batch_size, src_len).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If input tensor shapes or dimensions do not match expected values.\n",
    "        \"\"\"\n",
    "        # Validate input shapes\n",
    "        if encoder_outputs.dim() != 3:\n",
    "            raise ValueError(\n",
    "                f\"Expected encoder_outputs to be 3D (batch_size, src_len, encoder_hidden_dim), got {encoder_outputs.shape}\"\n",
    "            )\n",
    "        if decoder_hidden.dim() != 2:\n",
    "            raise ValueError(\n",
    "                f\"Expected decoder_hidden to be 2D (batch_size, decoder_hidden_dim), got {decoder_hidden.shape}\"\n",
    "            )\n",
    "\n",
    "        batch_size, src_len, enc_dim = encoder_outputs.size()\n",
    "        if enc_dim != self.encoder_hidden_dim:\n",
    "            raise ValueError(f\"Encoder hidden dimension mismatch: expected {self.encoder_hidden_dim}, got {enc_dim}\")\n",
    "        if decoder_hidden.size(1) != self.decoder_hidden_dim:\n",
    "            raise ValueError(f\"Decoder hidden dimension mismatch: expected {self.decoder_hidden_dim}, got {decoder_hidden.size(1)}\")\n",
    "        if batch_size != decoder_hidden.size(0):\n",
    "            raise ValueError(f\"Batch size mismatch: encoder_outputs {batch_size}, decoder_hidden {decoder_hidden.size(0)}\")\n",
    "\n",
    "        # Repeat decoder hidden state to match source length: (batch_size, decoder_hidden_dim) -> (batch_size, src_len, decoder_hidden_dim)\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "        # Compute energy: (batch_size, src_len, encoder_hidden_dim) -> (batch_size, src_len, attention_dim)\n",
    "        #                + (batch_size, src_len, decoder_hidden_dim) -> (batch_size, src_len, attention_dim)\n",
    "        energy = torch.tanh(self.encoder_attn(encoder_outputs) + self.decoder_attn(decoder_hidden))\n",
    "\n",
    "        # Compute attention scores: (batch_size, src_len, attention_dim) @ (attention_dim,) -> (batch_size, src_len)\n",
    "        attention_scores = torch.matmul(energy, self.v)\n",
    "\n",
    "        # Apply softmax to get attention weights: (batch_size, src_len)\n",
    "        attn_weights = F.softmax(attention_scores, dim=1)\n",
    "\n",
    "        # Compute context vector: (batch_size, 1, src_len) @ (batch_size, src_len, encoder_hidden_dim) -> (batch_size, 1, encoder_hidden_dim)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "\n",
    "        return context, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea41e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"Decoder with additive attention for sequence-to-sequence models.\n",
    "\n",
    "    Processes one token at a time, using attention to focus on relevant encoder outputs.\n",
    "    Outputs predictions for the next token and updated LSTM states.\n",
    "\n",
    "    Args:\n",
    "        output_dim (int): Size of the target vocabulary.\n",
    "        embed_dim (int): Dimension of token embeddings.\n",
    "        encoder_hidden_dim (int): Dimension of encoder hidden states.\n",
    "        decoder_hidden_dim (int): Dimension of decoder hidden states.\n",
    "        attention_dim (int): Dimension of the attention mechanism's hidden layer.\n",
    "        dropout (float, optional): Dropout probability. Defaults to 0.1.\n",
    "\n",
    "    Attributes:\n",
    "        embedding (nn.Embedding): Token embedding layer.\n",
    "        attention (AdditiveAttention): Attention mechanism.\n",
    "        rnn (nn.LSTM): LSTM layer for decoding.\n",
    "        fc_out (nn.Linear): Output projection layer.\n",
    "        dropout (nn.Dropout): Dropout layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim: int, embed_dim: int, encoder_hidden_dim: int, \n",
    "                 decoder_hidden_dim: int, attention_dim: int, dropout: float = 0.1):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.decoder_hidden_dim = decoder_hidden_dim\n",
    "        self.attention_dim = attention_dim\n",
    "\n",
    "        # Initialize layers\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.attention = AdditiveAttention(encoder_hidden_dim, decoder_hidden_dim, attention_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim + encoder_hidden_dim, decoder_hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(decoder_hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_token: torch.Tensor, decoder_hidden: torch.Tensor, \n",
    "                decoder_cell: torch.Tensor, encoder_outputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Performs one decoding step.\n",
    "\n",
    "        Args:\n",
    "            input_token (torch.Tensor): Input token IDs, shape (batch_size, 1).\n",
    "            decoder_hidden (torch.Tensor): Previous hidden state, shape (1, batch_size, decoder_hidden_dim).\n",
    "            decoder_cell (torch.Tensor): Previous cell state, shape (1, batch_size, decoder_hidden_dim).\n",
    "            encoder_outputs (torch.Tensor): Encoder outputs, shape (batch_size, src_len, encoder_hidden_dim).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                - prediction: Logits for next token, shape (batch_size, output_dim).\n",
    "                - hidden: Updated hidden state, shape (1, batch_size, decoder_hidden_dim).\n",
    "                - cell: Updated cell state, shape (1, batch_size, decoder_hidden_dim).\n",
    "                - attn_weights: Attention weights, shape (batch_size, src_len).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If input shapes do not match expected dimensions.\n",
    "        \"\"\"\n",
    "        # Validate input shapes\n",
    "        if input_token.dim() != 2 or input_token.size(1) != 1:\n",
    "            raise ValueError(f\"Expected input_token shape (batch_size, 1), got {input_token.shape}\")\n",
    "        if decoder_hidden.dim() != 3 or decoder_hidden.size(0) != 1:\n",
    "            raise ValueError(f\"Expected decoder_hidden shape (1, batch_size, decoder_hidden_dim), got {decoder_hidden.shape}\")\n",
    "        if decoder_cell.shape != decoder_hidden.shape:\n",
    "            raise ValueError(f\"Expected decoder_cell shape to match decoder_hidden, got {decoder_cell.shape}\")\n",
    "        if encoder_outputs.dim() != 3:\n",
    "            raise ValueError(f\"Expected encoder_outputs shape (batch_size, src_len, encoder_hidden_dim), got {encoder_outputs.shape}\")\n",
    "\n",
    "        batch_size = input_token.size(0)\n",
    "\n",
    "        # Embed input token: (batch_size, 1) -> (batch_size, 1, embed_dim)\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "\n",
    "        # Compute attention: (batch_size, src_len, encoder_hidden_dim), (batch_size, decoder_hidden_dim)\n",
    "        # -> (batch_size, encoder_hidden_dim), (batch_size, src_len)\n",
    "        context, attn_weights = self.attention(encoder_outputs, decoder_hidden.squeeze(0))\n",
    "        context = context.unsqueeze(1)  # (batch_size, 1, encoder_hidden_dim)\n",
    "\n",
    "        # Concatenate embedding and context: (batch_size, 1, embed_dim + encoder_hidden_dim)\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        # LSTM: (batch_size, 1, embed_dim + encoder_hidden_dim) -> (batch_size, 1, decoder_hidden_dim)\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (decoder_hidden, decoder_cell))\n",
    "\n",
    "        # Predict next token: (batch_size, 1, decoder_hidden_dim) -> (batch_size, output_dim)\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "\n",
    "        return prediction, hidden, cell, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f25c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Sequence-to-Sequence model combining encoder and decoder.\"\"\"\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, teacher_forcing_ratio: float = 0.5) -> torch.Tensor:\n",
    "        batch_size, tgt_len = tgt.size()\n",
    "        outputs = torch.zeros(batch_size, tgt_len, self.decoder.output_dim).to(src.device)\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1).unsqueeze(0)\n",
    "        cell = torch.cat((cell[-2], cell[-1]), dim=1).unsqueeze(0)\n",
    "        input_token = tgt[:, 0:1]\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell, _ = self.decoder(input_token, hidden, cell, encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            input_token = tgt[:, t:t+1] if torch.rand(1).item() < teacher_forcing_ratio else output.argmax(1, keepdim=True)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24440cb0",
   "metadata": {},
   "source": [
    "FOR epoch in range(num_epochs): \n",
    "    SET model to training mode\n",
    "    INIT loss to 0\n",
    "    FOR each batch (src, tgt) in dataloader:\n",
    "        Move data to device\n",
    "        Zero out gradients\n",
    "        Forward pass through model â†’ Get output\n",
    "        Calculate loss using output and target\n",
    "        Backpropagate\n",
    "        Clip gradients\n",
    "        Update weights using optimizer\n",
    "        Track loss\n",
    "    END\n",
    "    Log average loss\n",
    "SAVE model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02814c19",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 22)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<tokenize>:22\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mavg_loss = epoch_loss / len(dataloader)  # Average loss per batch\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def train_model(model: nn.Module, dataloader: DataLoader, optimizer: optim.Optimizer, \n",
    "                criterion: nn.Module, num_epochs: int, device: torch.device, save_path: str):\n",
    "    \"\"\"Train the Seq2Seq model with progress tracking and logging.\"\"\"\n",
    "    try:\n",
    "        model.to(device)  # Move model to GPU/CPU\n",
    "        for epoch in range(num_epochs):\n",
    "           model.train()  # Set to training mode (enables dropout)\n",
    "           epoch_loss = 0\n",
    "           with tqdm(total=len(dataloader), desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "                for src, tgt in dataloader:  # Iterate over batches\n",
    "                    src, tgt = src.to(device), tgt.to(device)  # Move data to device\n",
    "                    optimizer.zero_grad()  # Clear previous gradients\n",
    "                    output = model(src, tgt, Config().teacher_forcing_ratio)  # Forward pass\n",
    "                    loss = criterion(output[:, 1:].reshape(-1, model.decoder.output_dim), \n",
    "                                   tgt[:, 1:].reshape(-1))  # Compute loss\n",
    "                    loss.backward()  # Backpropagate\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
    "                    optimizer.step()  # Update weights\n",
    "                    epoch_loss += loss.item()  # Track loss\n",
    "                    pbar.update(1)  # Update progress bar\n",
    "                    pbar.set_postfix(loss=loss.item())  # Show current loss\n",
    "            avg_loss = epoch_loss / len(dataloader)  # Average loss per batch\n",
    "            logging.info(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}')  # Log result\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}')  # Print result\n",
    "        torch.save(model.state_dict(), save_path)  # Save model weights\n",
    "        logging.info(f'Model saved to {save_path}')  # Log save action\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training failed: {e}\")  # Log errors\n",
    "        raise  # Re-raise for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc21ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function for translation\n",
    "def translate_sentence(model: nn.Module, sentence: str, src_vocab, tgt_vocab, \n",
    "                      device: torch.device, max_length: int = 50) -> str:\n",
    "    \"\"\"Translate a sentence using the trained Seq2Seq model.\"\"\"\n",
    "    try:\n",
    "        model.eval()\n",
    "        tokens = ['<sos>'] + tokenize_en(sentence) + ['<eos>']\n",
    "        src_tensor = torch.tensor([src_vocab.get(token, src_vocab['<unk>']) for token in tokens], \n",
    "                                device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
    "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1).unsqueeze(0)\n",
    "            cell = torch.cat((cell[-2], cell[-1]), dim=1).unsqueeze(0)\n",
    "            input_token = torch.tensor([tgt_vocab['<sos>']], device=device).unsqueeze(0)\n",
    "            outputs = []\n",
    "            for _ in range(max_length):\n",
    "                output, hidden, cell, _ = model.decoder(input_token, hidden, cell, encoder_outputs)\n",
    "                pred_token = output.argmax(1).item()\n",
    "                if pred_token == tgt_vocab['<eos>']:\n",
    "                    break\n",
    "                outputs.append(pred_token)\n",
    "                input_token = torch.tensor([[pred_token]], device=device)\n",
    "        translation = ' '.join([tgt_vocab.get_itos()[idx] for idx in outputs])\n",
    "        return translation\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Inference failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b70f7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Main execution\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     args = \u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     config = Config()\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Load data\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mparse_args\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_args\u001b[39m():\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse command-line arguments.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     parser = \u001b[43margparse\u001b[49m.ArgumentParser(description=\u001b[33m'\u001b[39m\u001b[33mSeq2Seq Model Training and Inference\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m     parser.add_argument(\u001b[33m'\u001b[39m\u001b[33m--train\u001b[39m\u001b[33m'\u001b[39m, action=\u001b[33m'\u001b[39m\u001b[33mstore_true\u001b[39m\u001b[33m'\u001b[39m, help=\u001b[33m'\u001b[39m\u001b[33mTrain the model\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m     parser.add_argument(\u001b[33m'\u001b[39m\u001b[33m--infer\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m=\u001b[38;5;28mstr\u001b[39m, help=\u001b[33m'\u001b[39m\u001b[33mTranslate a sentence\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "# Command-line interface\n",
    "def parse_args():\n",
    "    \"\"\"Parse command-line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Seq2Seq Model Training and Inference')\n",
    "    parser.add_argument('--train', action='store_true', help='Train the model')\n",
    "    parser.add_argument('--infer', type=str, help='Translate a sentence')\n",
    "    parser.add_argument('--load_model', type=str, default=None, help='Path to pre-trained model')\n",
    "    return parser.parse_args()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    config = Config()\n",
    "    \n",
    "    # Load data\n",
    "    train_dataloader, src_vocab, tgt_vocab = get_dataloader(split='train', \n",
    "                                                          batch_size=config.batch_size, \n",
    "                                                          max_seq_len=config.max_seq_len)\n",
    "    \n",
    "    # Model instantiation\n",
    "    encoder = BiLSTMEncoder(\n",
    "        vocab_size=len(src_vocab),\n",
    "        embedding_dim=config.embedding_dim,\n",
    "        hidden_dim=config.hidden_dim,\n",
    "        num_layers=config.num_layers,\n",
    "        dropout=config.dropout\n",
    "    )\n",
    "    decoder = DecoderWithAttention(\n",
    "        output_dim=len(tgt_vocab),\n",
    "        embed_dim=config.embedding_dim,\n",
    "        encoder_hidden_dim=config.hidden_dim * 2,  # Bidirectional\n",
    "        decoder_hidden_dim=config.hidden_dim * 2,  # Match encoder's output\n",
    "        attention_dim=config.attention_dim,\n",
    "        dropout=config.dropout\n",
    "    )\n",
    "    model = Seq2Seq(encoder, decoder)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab['<pad>'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    # Load pre-trained model if specified\n",
    "    if args.load_model and os.path.exists(args.load_model):\n",
    "        model.load_state_dict(torch.load(args.load_model))\n",
    "        logging.info(f\"Loaded pre-trained model from {args.load_model}\")\n",
    "    \n",
    "    # Training\n",
    "    if args.train:\n",
    "        train_model(model, train_dataloader, optimizer, criterion, \n",
    "                   config.num_epochs, config.device, config.model_save_path)\n",
    "    \n",
    "    # Inference\n",
    "    if args.infer:\n",
    "        if not os.path.exists(config.model_save_path) and not args.load_model:\n",
    "            raise FileNotFoundError(\"No trained model found. Please train the model first or specify a pre-trained model path.\")\n",
    "        if args.load_model:\n",
    "            model.load_state_dict(torch.load(args.load_model))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(config.model_save_path))\n",
    "        translation = translate_sentence(model, args.infer, src_vocab, tgt_vocab, config.device)\n",
    "        print(f'Translation: {translation}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
